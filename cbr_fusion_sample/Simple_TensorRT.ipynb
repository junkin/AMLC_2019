{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through implementing a basic network using TensorRT's python API. The network is simple and illustrated layer fusion, tactic selection, concat ilision and mixed precision optimizations\n",
    "\n",
    "First we will import the needed python libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "import tensorrt as trt\n",
    "import numpy\n",
    "import sys, os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], \"..\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a dummy model, it doesnt have a purpose other than illustrating some of the underlying TensorRT concepts.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Change the logger severity to control what messages are displayed.\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "\n",
    "BIAS = 64\n",
    "\n",
    "class ModelData(object):\n",
    "    INPUT_NAME = \"data\"\n",
    "    INPUT_SHAPE = (1,3,224,224)\n",
    "    OUTPUT_NAME = \"prob\"\n",
    "    OUTPUT_SIZE = 1000\n",
    "    DTYPE = trt.float32\n",
    "\n",
    "#create a random simple network to explore layer fusion and scale fusion\n",
    "def add_cbr(network, input_tensor):\n",
    "    conv1_w = trt.Weights(numpy.random.rand(64,3,7,7).astype(np.float32))\n",
    "    conv1_b = trt.Weights(np.random.rand(64,).astype(np.float32))\n",
    "    conv1 = network.add_convolution(input=input_tensor, num_output_maps=64, kernel_shape=(7,7), kernel=conv1_w, bias=conv1_b)\n",
    "    conv1.stride = (2,2)\n",
    "    conv1.padding = (3,3)\n",
    "    #add a constant_layer here:\n",
    "    fc_bias = network.add_constant((1, 64, 112, 112), trt.Weights(numpy.random.rand(1, 64, 112, 112).astype(np.float32)))\n",
    "    bias1 = network.add_elementwise(\n",
    "                     conv1.get_output(0), \n",
    "                     fc_bias.get_output(0), \n",
    "                     trt.ElementWiseOperation.PROD) \n",
    "    relu1 = network.add_activation(input=bias1.get_output(0), type=trt.ActivationType.RELU)\n",
    "    return relu1\n",
    "\n",
    "#build a network using the TRT python api\n",
    "#we will add some layers to fuse, layers to eliminated (dead layers)\n",
    "#and layers to optimize with mixed precision\n",
    "def populate_network(network):\n",
    "    # Configure the network layers based on the weights provided.\n",
    "    input_tensor = network.add_input(name=ModelData.INPUT_NAME, dtype=ModelData.DTYPE, shape=ModelData.INPUT_SHAPE)\n",
    "\n",
    "    #add initial cbr block to the network:\n",
    "    relu1 = add_cbr(network, input_tensor)\n",
    "    \n",
    "    #Add a dead layer - we just wont use the output of cbr_output1 anwyere.  \n",
    "    #this can be made a useable layer by adding cbr_output1.get_output(0) to outputs \n",
    "    cbr_output1 = add_cbr(network, input_tensor)\n",
    "        \n",
    "    \n",
    "    #add a layer to concat\n",
    "    cbr_output2 = add_cbr(network, input_tensor)\n",
    "    \n",
    "    #collect the outputs for the concat layer\n",
    "    outputs = []\n",
    "    outputs.append(relu1.get_output(0))\n",
    "    outputs.append(cbr_output2.get_output(0))\n",
    "    \n",
    "    #uncomment after first run\n",
    "    #for i in range(10):\n",
    "    #  outputs.append(add_cbr(network, input_tensor).get_output(0))\n",
    "       \n",
    "    concatLayer = network.add_concatenation(outputs)\n",
    "    network.mark_output(tensor=concatLayer.get_output(0))\n",
    "\n",
    "def build_engine(enable_fp16=False):\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network:\n",
    "        builder.max_workspace_size = 1 << 30\n",
    "        \n",
    "        if(builder.platform_has_fast_fp16 and enable_fp16):\n",
    "            print(\"fast fp16 enabled\")\n",
    "            builder.fp16_mode=True\n",
    "   \n",
    "        # Populate the network with some random data and layers\n",
    "        # which demonstrate layer fusion, dead layer elimination, and horizontal fusion.\n",
    "        populate_network(network)\n",
    "        # Build and return an engine.\n",
    "        return builder.build_cuda_engine(network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the TRT engine - we need a different engine for each precision type we plan on using. For this example there are only two - fp32|fp16.  Then we serialize the optimized model onto the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engine = build_engine()\n",
    "#serialize engine\n",
    "with open('cbr_engine', 'wb') as f:\n",
    "   f.write(engine.serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the terminal screen and read through the output from the build_engine() function. There should be things like:\n",
    "\n",
    "[TensorRT] INFO: Original: 13 layers\n",
    "[TensorRT] INFO: After dead-layer removal: 9 layers\n",
    "...\n",
    "[TensorRT] INFO: After vertical fusions: 5 layers\n",
    "...\n",
    "[TensorRT] INFO: After tensor merging: 4 layers\n",
    "\n",
    "\n",
    "and the graph optimization section finishes with a section like:\n",
    "[TensorRT] INFO: After concat removal: 3 layers\n",
    "[TensorRT] INFO: Graph construction and optimization completed in 0.00046373 seconds.\n",
    "\n",
    "Which shows the final layer count - 3 in this case from 13.\n",
    "\n",
    "\n",
    "\n",
    "and the timing routines:\n",
    "[TensorRT] INFO: --------------- Timing (Unnamed Layer* 0) [Convolution] || (Unnamed Layer* 8) [Convolution](2)\n",
    "[TensorRT] INFO: Tactic 1 time 0.118112\n",
    "[TensorRT] INFO: Tactic 49 time 0.119808\n",
    "[TensorRT] INFO: Tactic 128 time 0.12016\n",
    "\n",
    "\n",
    "Go back up to the function populate_network and uncomment the for loop which adds 10 additional layers and reexcute that cell and the build engine cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an engine with fp16\n",
    "enginefp16 = build_engine(enable_fp16=True)\n",
    "with open('cbr_engine16', 'wb') as f2:\n",
    "    f2.write(enginefp16.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!ls cbr_engine*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(engine.device_memory_size)\n",
    "for i in range(engine.num_bindings):\n",
    "    print(engine.get_binding_shape(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create two helper functions - allocate_buffers and do_inference to move data to and from the engine once it has been loaded into the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE = trt.float32\n",
    "\n",
    "#We need to provide memory locations to move data between the gpu and the system memory\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "def allocate_buffers(engine):\n",
    "    h_input = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=trt.nptype(DTYPE))\n",
    "    h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=trt.nptype(DTYPE))\n",
    "    d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "    stream = cuda.Stream()\n",
    "    return h_input, d_input, h_output, d_output, stream\n",
    "\n",
    "\n",
    "def do_inference(context, h_input, d_input, h_output, d_output,stream):\n",
    "    # Transfer input data to the GPU.\n",
    "    #cuda.memcpy_htod(d_input, h_input)\n",
    "    #Try it async\n",
    "    cuda.memcpy_htod_async(d_input, h_input, stream)\n",
    "    # Run inference.\n",
    "    st = time.time()\n",
    "    #change from sync to async\n",
    "    #context.execute_async(batch_size=1, bindings=[int(d_input), int(d_output)], stream_handle = stream.handle)\n",
    "    #stream.synchronize()\n",
    "    context.execute(batch_size=1, bindings=[int(d_input), int(d_output)])\n",
    "    print('Inference time: {} [msec]'.format((time.time() - st)*1000))\n",
    "    # Transfer predictions back from the GPU.\n",
    "    #cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "\n",
    "    return h_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets allocate some buffers for input and output to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " h_input, d_input, h_output, d_output,stream = allocate_buffers(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create a randomly initialized tensor for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = np.random.rand(1,3,224,224).astype(numpy.float32)\n",
    "print(input.nbytes)\n",
    "print(trt.volume(engine.get_binding_shape(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_file16 = \"cbr_engine16\"\n",
    "\n",
    "#make sure we pull from the prebuilt engine\n",
    "with open(engine_file16, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine16 = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "input = np.random.rand(1,3,224,224).astype(numpy.float32).flatten()\n",
    "with engine16.create_execution_context() as context:\n",
    "    for i in range(10):\n",
    "      #input = np.random.rand(1,3,224,224).astype(numpy.float32).flatten()\n",
    "      np.copyto(h_input, input)\n",
    "      output = do_inference(context, h_input, d_input, h_output, d_output,stream)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_file = \"cbr_engine\"\n",
    "with open(engine_file, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine = runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "with engine.create_execution_context() as context:\n",
    "    for i in range(10):\n",
    "      #input = np.random.rand(1,3,224,224).astype(numpy.float32).flatten()\n",
    "      np.copyto(h_input, input)\n",
    "      output = do_inference(context, h_input, d_input, h_output, d_output,stream)\n",
    "      \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will show how to create profile dump for use with NSight System. NSight System can be downlaoded here https://developer.nvidia.com/nsight-systems.  This will be very useful to understand the whats and whys of model performance on the system.\n",
    "\n",
    "First we will want to clean up any old work, then we execute the nsys profile command line, first for the fp32 engine we have serialized, then for the fp16 implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm cbrengine_fp32*\n",
    "!nsys profile --show-output true --output cbrengine_fp32 --trace osrt,cuda,cudnn,cublas,nvtx  python do_inference.py cbr_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm cbrengine_fp16*\n",
    "!nsys profile --show-output true --output cbrengine_fp16 --trace osrt,cuda,cudnn,cublas,nvtx  python do_inference.py cbr_engine16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to download these files remember this is executing in a container, and they are located in the directory expoesed to the container here ~/ubuntu/amlc-2019/cbr_fusion_sample/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you want to download nsight compute GUI https://developer.nvidia.com/nsight-compute. This is not required to complete the notebook but will be a useful tool for model optimization, but will allow you to look at the profile create by the nsys command line tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nv-nsight-cu-cli -k scale -s 11 -c 1 '/usr/bin/python' do_inference.py cbr_engine16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nv-nsight-cu-cli --target-processes all -k trt_volta_scudnn_128x64_relu_medium_nn_v1 -s 11 -c 1 '/usr/bin/python' do_inference.py cbr_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
